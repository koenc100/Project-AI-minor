{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import statistics\n",
    "from oversampling import one_hot, smote_loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "from data_processing import prepare_data, split_data\n",
    "from helper_functions import get_metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, preprocessing, Input, metrics, initializers\n",
    "from tensorflow.keras.metrics import FalsePositives, TruePositives, FalseNegatives, TrueNegatives\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and clean it\n",
    "data = prepare_data('healthcare-dataset-stroke-data.csv')\n",
    "#data.replace({0: -1})\n",
    "# Split the data into test, training and validation data\n",
    "train_data, test_data, val_data, train_labels, test_labels, val_labels = split_data(data, split_size=(0.6, 0.2, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual model\n",
    "\n",
    "def train_and_predict(model, training_data, training_labels, \n",
    "                      testing_data, testing_labels, epochs=5, \n",
    "                      class_weight=10, verbose=0, plot=True):\n",
    "    \"\"\"\n",
    "    This function trains a given neural network model based on training data and training labels. It then predicts classes on\n",
    "    training and testing data. \n",
    "    It is possible to adjust for how many epochs the model is trained and how to weight the sparse class.\n",
    "    \n",
    "    input:\n",
    "    \n",
    "    model:        model architecture defined before calling this function\n",
    "    class_weight:  errors on the stroke class should be weighted heavier then the non-stroke class. \n",
    "                  The value defines how much more this loss is weighted. loss_weight=10 means a ratio of 1 to 10.\n",
    "                  For some reason\n",
    "    verbose:      0: no text per epoch\n",
    "                  1: text for each epoch\n",
    "    plot:         True: show accuracy and loss over epochs in figure\n",
    "                  False: no plot\n",
    "                  \n",
    "    output: \n",
    "    \n",
    "    predictions_train: vector of training predictions\n",
    "    predictions_test:  vector of test predictions\n",
    "    history:           dict containing measures over epochs, including loss, accuracy, TP, FP, TN, FN, for train and test data.\n",
    "                       print history.history for all measures and their keys.\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    # Compile the layers of the model defined earlier. Use the binary cross entropy function as the loss function as we only\n",
    "    # have 2 output classes and use accuracy as the metric\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # set warnings off (annoying bug in tensorflow)\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    # Train the model for a number of epochs\n",
    "    history = model.fit(training_data, training_labels, epochs=epochs, \n",
    "                        validation_data=(testing_data, testing_labels),\n",
    "                        class_weight=[{0: 1., 1: class_weight}],\n",
    "                        #sample_weight=[None],\n",
    "                        verbose=verbose\n",
    "                       )\n",
    "    # set warnings on again \n",
    "    tf.get_logger().setLevel('INFO')\n",
    "\n",
    "    # Predict the classes of the training data\n",
    "    predictions_train = model.predict(training_data) >= 0.5\n",
    "    \n",
    "    # Predict the classes on the testing data\n",
    "    predictions_test = model.predict(testing_data) >= 0.5\n",
    "    \n",
    "    # Plot the loss and accuracy over epochs.\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        fig.suptitle('loss and accuracy')\n",
    "    \n",
    "        axs[0].plot(history.history['accuracy'])\n",
    "        axs[0].plot(history.history['val_accuracy'])\n",
    "        axs[0].legend(['train', 'test'], loc='upper left')\n",
    "        axs[0].set_title('accuracy')\n",
    "        axs[0].set_ylabel('accuracy')\n",
    "        axs[0].set_xlabel('epochs')\n",
    "\n",
    "\n",
    "        # Plot the loss over epochs\n",
    "        axs[1].plot(history.history['loss'])\n",
    "        axs[1].plot(history.history['val_loss'])\n",
    "        axs[1].set_title('loss')\n",
    "        axs[1].set_xlabel('epochs')\n",
    "        axs[1].set_ylabel('loss')\n",
    "        axs[1].legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return predictions_train, predictions_test, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # Get the amount of input features for the nodes in the first layer\n",
    "    input_shape = np.shape(train_data)[1]\n",
    "\n",
    "    # Create the model\n",
    "    initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=12345)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First layer with input nodes equal to features\n",
    "    model.add(Input(shape=(input_shape)))\n",
    "\n",
    "    # One hidden layer with 25 nodes\n",
    "    model.add(layers.Dense(25, activation='relu', kernel_initializer=initializer))\n",
    "\n",
    "    model.add(layers.Dense(10, activation='relu', kernel_initializer=initializer))\n",
    "\n",
    "    # Output layer with 1 node (only 1 output class, 0 or 1 for stroke) and sigmoid activation function\n",
    "    model.add(layers.Dense(1, 'sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Train and predict\n",
    "predictions_train, predictions_test, history = train_and_predict(model, train_data, \n",
    "                                                        train_labels, test_data, test_labels,\n",
    "                                                        class_weight = 10,\n",
    "                                                        epochs=30, verbose=0)\n",
    "\n",
    "# Print metrics\n",
    "print('train metrics: \\n')\n",
    "accuracy_train, balanced_accuracy_train = get_metrics(train_labels, predictions_train, verbose=True)\n",
    "\n",
    "print('test metrics: \\n')\n",
    "accuracy_test, balanced_accuracy_test = get_metrics(test_labels, predictions_test, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different class weights and plot accuracy and sensitivity\n",
    "\n",
    "accuracies_val = []\n",
    "sensitivities_val = []\n",
    "balanced_accuracies_val = []\n",
    "specificities_val = []\n",
    "\n",
    "for i in tqdm(range(1, 30, 2)):\n",
    "    \n",
    "    # Train and predict\n",
    "    predictions_train, predictions_val, history = train_and_predict(model, train_data, \n",
    "                                                          train_labels, val_data, val_labels,\n",
    "                                                          epochs=30, verbose=0, plot=False,\n",
    "                                                          class_weight=i)\n",
    "    \n",
    "    # metrics\n",
    "    accuracy_val, balanced_accuracy_val = get_metrics(val_labels, predictions_val, verbose=False)\n",
    "    conmat = confusion_matrix(test_labels, predictions_val)\n",
    "    sensitivity = conmat[1,1] / sum(conmat[1,:])\n",
    "    specificity = conmat[0,0] / sum(conmat[0,:])\n",
    "\n",
    "    accuracies_val.append(accuracy_val)\n",
    "    balanced_accuracies_val.append(balanced_accuracy_val)\n",
    "    sensitivities_val.append(sensitivity)\n",
    "    specificities_val.append(specificity)\n",
    "    \n",
    "## Plot metrics over  class weights.\n",
    "\n",
    "plt.plot(range(1,30,2), accuracies_val)\n",
    "plt.plot(range(1,30,2), balanced_accuracies_val)\n",
    "plt.plot(range(1,30,2), sensitivities_val)\n",
    "plt.plot(range(1,30,2), specificities_val)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Effect of class weights on different output metrics in validation data')\n",
    "plt.ylabel('proportions')\n",
    "plt.xlabel('Relative weight of stroke class')\n",
    "plt.legend(['accuracy', 'balanced_accuracy', 'sensitivity', 'specificity'])\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and clean it\n",
    "data = prepare_data('healthcare-dataset-stroke-data.csv', one_hot = False, binary = True, normalize = True)\n",
    "\n",
    "#data.replace({0: -1})\n",
    "# Split the data into test, training and validation data\n",
    "train_data, test_data, val_data, train_labels, test_labels, val_labels = split_data(data, split_size=(0.6, 0.2, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tryout with different oversampling ratios\n",
    "\n",
    "oversampling_val = []\n",
    "oversampling_val_bal = []\n",
    "\n",
    "# Define categorial features\n",
    "n_features = np.array([True, False, True, True, True, True,True, False, False, True])\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "list_data, list_labels, list_ratio = smote_loop(train_data, train_labels, n_features, 0.2, 1.01, 0.2)\n",
    "\n",
    "for data_res, labels_res in zip(list_data, list_labels):\n",
    "    \n",
    "    # Train and predict\n",
    "    predictions_train, predictions_val, history = train_and_predict(model, data_res, \n",
    "                                                          labels_res, val_data, val_labels,\n",
    "                                                          epochs=30, verbose=0, plot=False,\n",
    "                                                          class_weight=[{0: 1., 1: 10}])\n",
    "    \n",
    "    # metrics\n",
    "    accuracy_val, balanced_accuracy_val = get_metrics(val_labels, predictions_val, verbose=False)\n",
    "\n",
    "    oversampling_val.append(accuracy_val)\n",
    "    oversampling_val_bal.append(balanced_accuracy_val)\n",
    "    \n",
    "## Plot metrics over  class weights.\n",
    "\n",
    "plt.plot(np.linspace(0.2, 1, 5), accuracies_val)\n",
    "plt.plot(np.linspace(0.2, 1, 5), balanced_accuracies_val)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Effect of class weights on different output metrics in validation data')\n",
    "plt.ylabel('proportions')\n",
    "plt.xlabel('Relative weight of stroke class')\n",
    "plt.legend(['accuracy', 'balanced_accuracy'])\n",
    "plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(X, y, k=5, class_weight=10, epochs=5, to_return=['metrics', 'history']):\n",
    "    \"\"\"\n",
    "    Uses k-fold cross-validation to estimate average training and validation accuracies over different splits.\n",
    "    Uses all training/validation data and resplits it 5 times into a training and validation set.\n",
    "    Then performs training and testing of the model and takes average of accuracies over all 5 folds.\n",
    "    \n",
    "    input:\n",
    "    X:              data array containing all features of training and validation data for cross-validation\n",
    "    y:              labels of X data\n",
    "    k:              number of folds for cross-validation\n",
    "    class_weight:   relative weight of stroke class cost for model training (ratio 1:class_weight)\n",
    "    epochs:         number of epochs\n",
    "    to_return:      list with either one or two strings:\n",
    "                    'metrics': returns all relevant metrics after all epochs are finished, averaged over k splits\n",
    "                    'history': returns the loss and accuracy of train and validation data after each epoch, \n",
    "                               averaged over k splits\n",
    "    \n",
    "    output:         optional: metrics, history (both tuples)\n",
    "                    metrics: avg_acc_train, avg_bal_acc_train, avg_acc_val, avg_bal_acc_val, avg_sens, avg_spec\n",
    "                    history: avg_loss_over_epochs, avg_acc_over_epochs, avg_loss_over_epochs_val, avg_acc_over_epochs_val\n",
    "    \n",
    "    \"\"\"\n",
    "    # ensure that something is returned, also if one of these will not be calculated.\n",
    "    metrics = None\n",
    "    history = None\n",
    "    \n",
    "    # initialize metrics\n",
    "    if 'metrics' in to_return:\n",
    "        accuracies_train = []\n",
    "        balanced_accuracies_train = []\n",
    "        accuracies_val = []\n",
    "        balanced_accuracies_val = []\n",
    "        sensitivities = []\n",
    "        specificities = []\n",
    "        histories = []\n",
    "    \n",
    "    # get object that splits the data in 5 equal stratified folds.\n",
    "    sss = StratifiedShuffleSplit(n_splits=k, test_size=0.2, random_state=0)\n",
    "    # loop over those 5 datasets\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        x_train, x_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "        \n",
    "        # Find the best fitting theta for this training set using the Normal equation\n",
    "        predictions_train, predictions_val, history = train_and_predict(model, x_train, y_train, \n",
    "                                          x_val, y_val, epochs=epochs, \n",
    "                                          class_weight=class_weight, verbose=0, plot=False)\n",
    "        \n",
    "\n",
    "        if 'history' in to_return:\n",
    "            # store history of each split\n",
    "            histories.append(history)\n",
    "        \n",
    "        if 'metrics' in to_return:\n",
    "            # get metrics for this split\n",
    "            accuracy_train, balanced_accuracy_train = get_metrics(y_train, predictions_train, verbose=False)\n",
    "            accuracy_val, balanced_accuracy_val = get_metrics(y_val, predictions_val, verbose=False)\n",
    "            # get confusion matrix to calculate sensitivity and specificity.\n",
    "            conmat = confusion_matrix(y_val, predictions_val)\n",
    "            sensitivity = conmat[1,1] / sum(conmat[1,:])\n",
    "            specificity = conmat[0,0] / sum(conmat[0,:])\n",
    "            \n",
    "            # store metrics for this split\n",
    "            accuracies_train.append(accuracy_train)\n",
    "            balanced_accuracies_train.append(balanced_accuracy_train)\n",
    "            accuracies_val.append(accuracy_val)\n",
    "            balanced_accuracies_val.append(balanced_accuracy_val)\n",
    "            sensitivities.append(sensitivity)\n",
    "            specificities.append(specificity)\n",
    "    \n",
    "\n",
    "    if 'metrics' in to_return:\n",
    "        # run over all metrics and compute and store their mean over all k splits.\n",
    "        metrics_all = [accuracies_train, balanced_accuracies_train, accuracies_val, \n",
    "                       specificities, balanced_accuracies_val, sensitivities]\n",
    "        metrics = []\n",
    "        for metric in metrics_all:\n",
    "            # compute average metrics over all k splits.\n",
    "            metrics.append(statistics.mean(metric))\n",
    "        metrics = tuple(metrics)\n",
    "\n",
    "    if 'history' in to_return:\n",
    "        history_metrics = []\n",
    "        history_metrics_names = ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n",
    "        for metric in history_metrics_names:\n",
    "            # compute average loss and accuracy over all k splits for each epoch\n",
    "            metric_history = np.zeros(len(history.history[metric]))\n",
    "        \n",
    "            # loop over the history dictionaries for each k split.\n",
    "            for i in histories:\n",
    "                metric_history = metric_history + np.array(i.history[metric])\n",
    "                # add up data of each split for the loss and accuracy \n",
    "                # training data     \n",
    "                \n",
    "            history_metrics.append(metric_history / k)\n",
    "        history = tuple(history_metrics)\n",
    "    return metrics, history_metrics\n",
    "    \n",
    "\n",
    "# Datasets for k-fold cross-validation.\n",
    "data = prepare_data('healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "\n",
    "\n",
    "# get cross-validation data (which will be further split into train and validation data), and test data.\n",
    "X, test_data, y, test_labels = split_data(data, split_size=(0.8, 0.2))\n",
    "\n",
    "# perform 5-fold cross-validation\n",
    "epochs = 50\n",
    "metrics, history = k_fold_validation(np.array(X), np.array(y), k=5, class_weight=15, epochs=epochs, to_return=['metrics','history'])\n",
    "\n",
    "# unpack tuples\n",
    "avg_loss_over_epochs, avg_acc_over_epochs, avg_loss_over_epochs_val, avg_acc_over_epochs_val = history\n",
    "avg_acc_train, avg_bal_acc_train, avg_acc_val, avg_bal_acc_val, avg_sens, avg_spec = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss and accuracy during learning of the model, averaged over k splits using cross-validation\n",
    "epochs = len(avg_loss_over_epochs)\n",
    "plt.plot(range(1, epochs + 1), avg_loss_over_epochs)\n",
    "plt.plot(range(1, epochs + 1), avg_loss_over_epochs_val)\n",
    "plt.title('average loss over k splits during model training')\n",
    "plt.ylabel('average loss over k splits')\n",
    "plt.legend(['train data', 'validation data'], loc='upper right')\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, epochs + 1), avg_acc_over_epochs)\n",
    "plt.plot(range(1, epochs + 1), avg_loss_over_epochs_val)\n",
    "plt.title('average accuracy over k splits during model training')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('average accuracy over k splits')\n",
    "plt.legend(['train data', 'validation data'], loc='upper right')\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets for k-fold cross-validation.\n",
    "X = prepare_data('healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "# get cross-validation data (which will be further split into train and val data), and test data.\n",
    "X, test_data, y, test_labels = split_data(data, split_size=(0.8, 0.2))\n",
    "\n",
    "avg_accs = []\n",
    "avg_accs_bal = []\n",
    "avg_accs_val = []\n",
    "avg_accs_bal_val = []\n",
    "avg_sensitivities = []\n",
    "avg_specificities = []\n",
    "\n",
    "# do k-fold cross validation for different class weights\n",
    "for i in tqdm(range(1, 30, 2)):\n",
    "    metrics, history = k_fold_validation(np.array(X), np.array(y), k=5, \n",
    "                                         class_weight=i, epochs=5, to_return=['metrics', 'history'])\n",
    "\n",
    "    # unpack metrics\n",
    "    avg_acc_train, avg_bal_acc_train, avg_acc_val, avg_bal_acc_val, avg_sens, avg_spec = metrics\n",
    "\n",
    "    avg_accs.append(avg_acc_train)\n",
    "    avg_accs_bal.append(avg_bal_acc_train)\n",
    "    avg_accs_val.append(avg_acc_val)\n",
    "    avg_accs_bal_val.append(avg_bal_acc_val)\n",
    "    avg_sensitivities.append(avg_sens)\n",
    "    avg_specificities.append(avg_spec)\n",
    "    \n",
    "plt.title('Training data')\n",
    "plt.plot(range(1, 30, 2), avg_accs)\n",
    "plt.plot(range(1, 30, 2), avg_accs_bal)\n",
    "plt.ylabel('proportions')\n",
    "plt.xlabel('Relative weight of stroke class')\n",
    "plt.legend(['accuracy', 'balanced_accuracy'])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Validation data')\n",
    "plt.plot(range(1, 30, 2), avg_accs_val)\n",
    "plt.plot(range(1, 30, 2), avg_accs_bal_val)\n",
    "plt.plot(range(1, 30, 2), avg_sensitivities)\n",
    "plt.plot(range(1, 30, 2), avg_specificities)\n",
    "plt.ylabel('proportions')\n",
    "plt.xlabel('Relative weight of stroke class')\n",
    "plt.legend(['accuracy', 'balanced_accuracy', 'sensitivity', 'specificity'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
