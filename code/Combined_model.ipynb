{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823193ad",
   "metadata": {},
   "source": [
    "# Combined model\n",
    "\n",
    "In this notebook we will combine the optimized version of the three models we've created. So, the predictions of our k-Nearest Neighbor, Decision tree and Neural Network will be combined to one new prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instal nbimporter to be able to import functions from other notebooks\n",
    "pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d92eee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import prepare_data, split_data, one_hot_encode\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "from helper_functions import get_metrics\n",
    "\n",
    "# Import functions for k-nearest neighbors\n",
    "from kNN_die_wel_opent import split_datatypes, train_and_predict\n",
    "from oversampling import smote_loop\n",
    "\n",
    "# Import functions for decision tree\n",
    "from Decision_tree import resampled_forest\n",
    "\n",
    "# Load the data normalized\n",
    "data = prepare_data('healthcare-dataset-stroke-data.csv', one_hot = False, binary = False, normalize = True)\n",
    "\n",
    "# Load the data one-hot encoded and not normalized\n",
    "data_one_hot = prepare_data('healthcare-dataset-stroke-data.csv', one_hot = True, binary = False, normalize = False)\n",
    "\n",
    "# Split the normalized data into training, testing and validation data\n",
    "train_data, test_data, val_data, train_labels, test_labels, val_labels = split_data(data, (0.6, 0.2, 0.2))\n",
    "\n",
    "# Split the one-hot encoded data into training, testing and validation data\n",
    "train_hot, test_hot, val_hot, train_labels_hot, test_labels_hot, val_labels_hot = split_data(data_one_hot, (0.6, 0.2, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34943d00",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbor\n",
    "The k-Nearest Neighbor model with the best balanced accuracy was trained on only numeric data that was overfitted with a ratio of 0.6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eea78eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change testing data to one hot encoded data, since this also happens to training data in smote_loop\n",
    "test_data_hot = one_hot_encode(test_data)\n",
    "\n",
    "# Split test data into numeric and binary data\n",
    "test_num, test_bin = split_datatypes(test_data_hot)\n",
    "\n",
    "# Get the oversampled data with a oversampling ratio of 0.6\n",
    "data_list, labels_list, ratio_list = smote_loop(train_data, train_labels, 0.6, 0.7, 0.1)\n",
    "train_num, train_bin = split_datatypes(data_list[0])\n",
    "\n",
    "# Predictions using model trained on numerical, oversampled data and euclidean distance metric and 5 neighbors\n",
    "predict_train_kNN, predict_test_kNN = train_and_predict(train_num, labels_list[0], test_num, 5, \"distance\", \n",
    "                                                          metric='euclidean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99c3c6",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "The optimal number of splits was around 17 most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be91105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tuple of the data that gets accepted by the forest function\n",
    "data_DT = (train_hot, train_labels_hot, test_hot, test_labels_hot)\n",
    "\n",
    "# Train the forest on the training data and return a list with predicted labels fror training and testing data\n",
    "predict_train_DT, predict_test_DT = resampled_forest(data_DT, 17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdbb15",
   "metadata": {},
   "source": [
    "# Combining the models\n",
    "The models can be combined in different ways. Considering we started with too few stroke predictions an OR function might be good. We will also try to build and train a neural network based on the output of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9cda2739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using only k-Nearest Neighbors: \n",
      "accuracy: 83.4638 % \n",
      "\n",
      "balanced accuracy: 68.5412 %\n",
      "sensitivity: 0.5200\n",
      "specificity: 0.8508 \n",
      "\n",
      "confusion matrix: \n",
      "[[827 145]\n",
      " [ 24  26]] \n",
      "\n",
      "[[\"True Negative\", \"False Positive\"] \n",
      " [\"False Negative\", \"True Positive\"]] \n",
      "\n",
      "The accuracy using only Random Forest: \n",
      "accuracy: 73.5812 % \n",
      "\n",
      "balanced accuracy: 75.6770 %\n",
      "sensitivity: 0.7800\n",
      "specificity: 0.7335 \n",
      "\n",
      "confusion matrix: \n",
      "[[713 259]\n",
      " [ 11  39]] \n",
      "\n",
      "[[\"True Negative\", \"False Positive\"] \n",
      " [\"False Negative\", \"True Positive\"]] \n",
      "\n",
      "The accuracy using the combined predictions: \n",
      "accuracy: 71.0372 % \n",
      "\n",
      "balanced accuracy: 76.2366 %\n",
      "sensitivity: 0.8200\n",
      "specificity: 0.7047 \n",
      "\n",
      "confusion matrix: \n",
      "[[685 287]\n",
      " [  9  41]] \n",
      "\n",
      "[[\"True Negative\", \"False Positive\"] \n",
      " [\"False Negative\", \"True Positive\"]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict_combined = map(list(predict_test_kNN) and list(predict_test_DT))\n",
    "predict_combined_2 = np.any([predict_test_kNN, predict_test_DT], axis=0)\n",
    "\n",
    "print('The accuracy using only k-Nearest Neighbors: ')\n",
    "test_acc, test_balacc = get_metrics(test_labels, predict_test_kNN, verbose = True)\n",
    "print('The accuracy using only Random Forest: ')\n",
    "test_acc, test_balacc = get_metrics(test_labels, predict_test_DT, verbose = True)\n",
    "print('The accuracy using the combined predictions: ')\n",
    "test_acc, test_balacc = get_metrics(test_labels, predict_combined_2, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
